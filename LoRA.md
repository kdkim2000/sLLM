# π“‘ LoRA: Low-Rank Adaptation of Large Language Models μ”μ•½

## 1. μ†κ° (Introduction)

- **λ¬Έμ **: λ€κ·λ¨ λ¨λΈ(GPT-3 175B λ“±)μ ν’€νμΈνλ‹μ€ λ„λ¬΄ λ§μ€ μμ› μ†λ¨
- **LoRA μ μ•**: Pretrained λ¨λΈμ κ°€μ¤‘μΉλ” κ³ μ •ν•κ³ , κ° λ μ΄μ–΄μ— μ‘μ€ Low-Rank ν–‰λ ¬(A, B)μ„ μ‚½μ…ν•μ—¬ ν•™μµ
- **μ£Όμ” ν¨κ³Ό**: ν•™μµ νλΌλ―Έν„° μ 10,000λ°° μ κ°, λ©”λ¨λ¦¬ μ‚¬μ© 3λ°° μ κ°, μ„±λ¥μ€ ν’€νμΈνλ‹κ³Ό λ™λ“±ν•κ±°λ‚ μ°μ


## 2. λ¬Έμ  μ •μ (Problem Statement)

- κΈ°μ΅΄ ν’€νμΈνλ‹: λ¨λ“  λ¨λΈ νλΌλ―Έν„° μ—…λ°μ΄νΈ β†’ μ €μ¥/μ΄μ λΉ„ν¨μ¨
- LoRA: β†Ξ¦λ¥Ό μ†μ νλΌλ―Έν„°(Ξ)λ΅ μ¬κµ¬μ„±ν•μ—¬ ν•™μµ λΉ„μ© λ° μ¤ν† λ¦¬μ§€ μ κ°


## 3. κΈ°μ΅΄ λ°©λ²•λ΅  ν•κ³„ (Aren't Existing Solutions Good Enough?)

| λ°©λ²• | ν•κ³„ |
|:---|:---|
| Adapter Layers | μ¶”κ°€ λ μ΄μ–΄λ΅ μΈν•΄ μ¶”λ΅  μ§€μ—° μ¦κ°€ |
| Prompt νλ‹ | μµμ ν™” λ‚μ΄λ„ λ†’κ³ , μ…λ ¥ μ‹ν€€μ¤ κΈΈμ΄ κ°μ† |


## 4. LoRA λ°©λ²•λ΅  (Our Method)

- **Low-Rank Update**: κΈ°μ΅΄ Weight(W0)μ— λ€ν•΄ W0 + BA ν•μ‹μΌλ΅ μ—…λ°μ΄νΈ (r β‰ d)
- **Training**: A, Bλ§ ν•™μµ, W0λ” κ³ μ •
- **Inference**: W0 + BAλ¥Ό λ³‘ν•©ν•μ—¬ λ³„λ„ μ§€μ—°(latency) μ—†μ΄ μ‚¬μ© κ°€λ¥

### Transformer μ μ©
- Wq, Wv λ§¤νΈλ¦­μ¤μ— LoRA μ μ© (Self-Attention λ¶€λ¶„)
- ν•™μµ μ¤‘ VRAM μµλ€ 2/3 μ κ°, μ €μ¥κ³µκ°„ 10,000λ°° μ κ° κ°€λ¥


## 5. LoRA μ „μ²΄ κµ¬μ΅° κ·Έλ¦Ό

```mermaid
graph TD
OriginalModel[Pretrained Model Parameters -W0] -->|κ³ μ •| SumBlock[Sum with Low-Rank Updates]
LowRankUpdate[Low-Rank Matrices - A, B] --> SumBlock
SumBlock --> FineTunedModel[Fine-Tuned Output - W0 + BA]
```

- W0μ€ κ³ μ •λμ–΄ μκ³ , Aμ™€ B(μ €μ°¨μ› ν–‰λ ¬)λ§ ν•™μµν•μ—¬ μµμΆ… Weight(W0 + BA) μƒμ„±


## 6. μ‹¤ν— κ²°κ³Ό (Empirical Experiments)

| λ¨λΈ | κ²°κ³Ό |
|:---|:---|
| RoBERTa base/large | GLUE benchmarkμ—μ„ κΈ°μ΅΄ λ°©λ²• λ€λΉ„ μ°μ μ„±λ¥ |
| DeBERTa XXL | μ΄λ€ν• λ¨λΈμ—μ„λ„ μ„±λ¥ μ μ§€ |
| GPT-2 Medium/Large | μμ—°μ–΄ μƒμ„±(E2E, WebNLG, DART) μ„±λ¥ ν–¥μƒ |
| GPT-3 175B | WikiSQL, MNLI, SAMSumμ—μ„ ν’€νμΈνλ‹κ³Ό λΉ„μ·ν•κ±°λ‚ λ” λ‚μ€ μ„±λ¥ |


## 7. κ΄€λ ¨ μ—°κµ¬ (Related Works)

- Adapter Layers, Prompt Tuning λ°©λ²•λ“¤κ³Ό λΉ„κµ
- LoRAλ” μ¶”κ°€ μ¶”λ΅  μ§€μ—° μ—†μ΄ μ„±λ¥μ„ μ μ§€ν•λ‹¤λ” μ μ΄ μ°¨λ³„ν™” ν¬μΈνΈ


## 8. Low-Rank μ—…λ°μ΄νΈμ— λ€ν• μ‹¬μΈµ λ¶„μ„

- **μ μ© λ€μƒ**: Wq, Wv λ§¤νΈλ¦­μ¤μ— μ μ©ν•  λ• κ°€μ¥ μΆ‹μ€ μ„±λ¥
- **μ μ ν• Rank**: r=4 μ •λ„λ©΄ λ€λ¶€λ¶„ μΆ‹μ€ μ„±λ¥ (GPT-3 175B μ‹¤ν—)
- **β†Wμ™€ Wμ κ΄€κ³„**: β†Wλ” Wμ λ³΄μ΅°μ μΈ λ°©ν–¥μ„ μ¦ν­ν•λ” μ—­ν• 


## 9. LoRA vs Adapter vs Prefix Tuning λΉ„κµ

| λ°©λ²• | ν•™μµ νλΌλ―Έν„° μ | μ¶”λ΅  μ†λ„ | μ…λ ¥ λ³€κ²½ μ—¬λ¶€ | μ¥μ  |
|:---|:---|:---|:---|:---|
| Full Fine-Tuning | μ „μ²΄ | λλ¦Ό | λ³€κ²½ μ—†μ | μµκ³  μ„±λ¥ |
| Adapter Layers | μ μ | λ‹¤μ† λλ¦Ό | λ³€κ²½ μ—†μ | ν¨μ¨μ , κµ¬μ΅° λ‹¨μ |
| Prefix Tuning | λ§¤μ° μ μ | λΉ λ¦„ | μ…λ ¥ κΈΈμ΄ μ¦κ°€ | κ·Ήλ‹¨μ  ν¨μ¨μ„± |
| **LoRA** | λ§¤μ° μ μ | λΉ λ¦„ | λ³€κ²½ μ—†μ | ν¨μ¨ + μ§€μ—° μ—†μ |

> β… LoRAλ” μ¶”λ΅  μ‹κ°„μ— μν–¥μ„ κ±°μ μ£Όμ§€ μ•μΌλ©΄μ„ ν•™μµ ν¨μ¨μ„ λ†’μ΄λ” μµκ³ μ νƒ€ν‘μ•μΌλ΅ ν‰κ°€λ©λ‹λ‹¤.


## 10. κ²°λ΅  λ° λ―Έλ μ—°κµ¬ λ°©ν–¥

- **LoRA**λ” λ€κ·λ¨ λ¨λΈ μ μ‘μ„ μ„ν• κ°•λ ¥ν•κ³  ν¨μ¨μ μΈ λ°©λ²•
- ν–¥ν›„ λ°©ν–¥: λ‹¤λ¥Έ μ••μ¶•/νλ‹ κΈ°λ²•κ³Ό κ²°ν•©, Task-specific λ§¤νΈλ¦­μ¤ μ„ νƒ μλ™ν™” μ—°κµ¬ μμ •


---

# π“ μµμΆ… μ”μ•½ ν‚¤μ›λ“

- Low-Rank Adaptation
- PEFT (Parameter Efficient Fine Tuning)
- Memory Efficiency
- Transformer Fine-Tuning
- Adapter vs Prefix vs LoRA λΉ„κµ

---

